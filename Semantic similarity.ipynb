{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of semantic similarity\n",
    "\n",
    "- semantic similarity is useful when you're grouping similar words into semantic concepts that have the same meaning or appear to have the same meaning\n",
    "- it is very useful as a building block in natural language understading task; tasks such as textual entailment or paraphrasing\n",
    "- paraphrasing is a task where you rephrase or rewrite some sentece you get into another sentence that has the same meaning\n",
    "- textual entailment on the other hand is a little bit more complex. it says that the smaller sentence or one of the two sentences derives its meaning or entails its meaning from another piece of text.\n",
    "- one of the resources useful for semantic similarity is WordNet. WordNet is a semantic dictionary of words interlinked by semantic relationship\n",
    "- it is most extensively developed in english but there are WordNets available now for quite  few other languages\n",
    "- Wordet is also machine readable and its freely available so it is extensively used in a lot of Natural Language processing tasks and in general in text mining tasks\n",
    "\n",
    "\n",
    "        - How do you use WordNet for semantic similarity? WordNet organizes information in a hierarchy, in a tree. You have a dummy root that is on top of all words of the same part of speech. So noun has a dummy root. A verb has a dummy root. And then, there are many semantic similarity measures that are using this hierarchy, in some way. For example, you have different hierarchies for this part of speech. And let's take an example of our deer that we started with, where deer, elk, giraffe, horse and so on, these words are grouped together, in some form, in this hierarchy. For example, elk, wapiti, and caribou are all types of deer. Deer and giraffe are siblings in this tree hierarchy because they are ruminants, and so on. And horse's related but not in the same hierarchy. It's related because horse and deer are ungulates, but they are not siblings, for example. So one such measure of using this hierarchy for defining semantic similarity is path similarity. You could imagine that you would start with one of these concepts, and see how many steps you need to take to get to the other. In other words, you are finding a shortest path between these two concepts in this hierarchy. And then, similarity can be just measured as inversely related to this distance that we computed. For example, if you have deer and elk, you would have, the deer and elk, actually are, have a parent-child relationship in this case, so the distance is one, while deer and let's take in another color, deer and giraffe is the sense of two, because you need to go up ruminant and down giraffe, so you have a distance of two. In general, you can see that when we computed with paths you use this one, distance of one between deer and elk and say, it's one over the distance plus one, so one over two that's .5. The distance between a deer and giraffe is one over three, so that's, 0.33 and if you just measure the same way, going from deer to horse you'd say it's one, two, three, four, five, six. It's one over seven and that would be 0.14. The other way to find similarity between two concepts is using what is called lowest common subsumer. Lowest common subsumer is that ancestor that is closest to both concepts. For example, deer and giraffe have the least common or lowest common subsumer to be ruminants. You have deer and giraffe and you know that is the least common subsumer is the one that is an ancestor to both of them, but the lowest in the hierarchy. Even though ungulates, and even toward ungulate are both ancestors, it's the ruminant that is the lowest one in that hierarchy. With respect to deer and elk, it's just the deer because deer is a parent for elk so the one that subsumes both of these would be directive, and for deer and horse it goes all the way up to ungulate. Now, you can use this lowest common subsumer notion to find similarity and that was proposed by Lin and called Lin similarity. You have similarity measure that is based on the information contained in the lowest common subsumer of the two concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find appropriate sense of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deer = wn.synset('deer.n.01')\n",
    "elk = wn.synset('elk.n.01')\n",
    "horse = wn.synset('horse.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(elk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deer.path_similarity(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use an information criteria to find Lin similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8623778273893673\n",
      "0.7726998936065773\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "print(deer.lin_similarity(elk, brown_ic))\n",
    "print(deer.lin_similarity(horse, brown_ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations and Distributional similarity\n",
    "- collocations can be defined by this code; 'you know a word by the company it keeps'\n",
    "- that means two words that are frequently appearing in similar concept or in similar contexts are more likely to be similar or more likely to be semantically related\n",
    "- so if you have two words that keep appearing in very similar contexts or that could replace another word in the similar context and still the meaning remains the same, then there are more likely to be semantically related\n",
    "- lets take an example of four sentences;\n",
    "    - the friends *met at a* **cafe**\n",
    "    - shyam *met* Ray *at a* **pizzeria**\n",
    "    - let's *meet* up *near the* **coffee shop**\n",
    "    - the secret *meeting at the* **resturant** soon became public.\n",
    "- these words, cafe or pizzeria or coffee shop or resturant are semantically related because they typically occur around the words meet, around, at, or, near and the.\n",
    "- so there is a determinant right in front of them and there is some notion of location and those are concepts that would form your context around the word.\n",
    "-  In general, you would define context based on words before, after, or within a small window of a target word, so word what comes before. For example, for all of these was a cafe and restaurant and so on, it was 'a' or 'the', alright? Because it's a noun and you have a determiner right before that. \n",
    "- What comes after or what comes within a small window? Let's say, of size three and you will remember that all of those examples had some form of meet. Met, meet, meeting and so on in that small window of three to five words.\n",
    "- you could have some specific semantic relation to the target word or you could have words that come from the same sentence, in the same document, and you can define that document as any length you want, lets say a passage in a document, a paragraph that would constitute your context\n",
    "\n",
    "### strength of association between words\n",
    "- once you have defined this context you can compute the strength of association between words based on how frequently they collocate.\n",
    "- for example if you have two words that keep coming very close to each other you would want to say that they are very highly related to each other.\n",
    "- on the other side, if they dont occur together, then they are not necessarily very similar\n",
    "- its also important to see how frequent individual words are.\n",
    "- for example, the word 'the' is so frequent that it would occur with every other word, fairly often.\n",
    "- the similarity score would be very high with 'the' just because 'the' itself happens to be very frequent.\n",
    "- there is a way in which you can normalize such that this very frequent word does not kind of, superide all the other similarity measures you find.\n",
    "- one way to do this would be using pointwise mutual information.\n",
    "-  Pointwise Mutual Information is defined as the log of this ratio of seeing two things together. Seeing the word and the context together, divided by the probability of these occurring independently. What is the chance that you would see the world in the overall corpus? What is the chance that you can see the context word in the overall corpus and what is the chance that they are actually occurring together?\n",
    "- the pointwise mutual information is also something that you can directly call in the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e5a14183e1b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbigram_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram_measures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpmi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(text)# learn based on a corpus, here given as text corpus\n",
    "finder.nbest(bigram_measures.pmi, 10) #returns the top 10 pairs using the pmi measure from bigrams_measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frequency filter in Finder\n",
    "\n",
    "- You can use a Use Finder for other useful tasks such as frequency filtering. So suppose you want all bigram measures that are, there you have supposed 10 or more occurrences of words only then can you keep them, then you could do something like finder.apply_freq_filter(10). That would then restrict any pair that does not occur at least 10 times in your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-dbdbb73f96bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_freq_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'finder' is not defined"
     ]
    }
   ],
   "source": [
    "finder.apply_freq_filter(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take home concepts\n",
    "- finding similarity between words and text is non-trivial\n",
    "- WordNet is a useful resource for semantic relationships between words\n",
    "- many similarity functions exist\n",
    "- NLTK is a useful package for many such tasks\n",
    "\n",
    "\n",
    "\n",
    "## Topic modelling\n",
    "- topic modelling is a coarse-level analysis of what is in a text collection.\n",
    "- when you have a large corpus and you want to make sense of what this collection is about, you would probably use topic modelling\n",
    "- a topic is a subject or theme of a discourse.\n",
    "- topics are represented by a word distribution and that means you have a probability of a word appearing in that topic\n",
    "- a document is assumed to be a mixture of topics\n",
    "- when you are doing topic modelling, whats given to you is;\n",
    "    - text collection or corpus\n",
    "    - you are somehow given the numer of topics, lets say 20 topics\n",
    "- what is not given to you is;\n",
    "    - the actual topics\n",
    "    - you are also not given the distribution of these topics\n",
    "- essentially, topic modelling is a text clustering problem.\n",
    "- however, in this particular case, the documents and words are clustered simultaneously.\n",
    "- you need to figure out which words come together, how are they similar to each other or semantically related to each other.\n",
    "- you also need to figure out what documents come together, what documents are of the same topic or mostly abou the same topic\n",
    "- and how does these words get derived based on these documents\n",
    "- there are different topic modelling approaches available and there have been new models that are defined very regularly in computer science literature.\n",
    "- the most common ones and the ones that started this field are;\n",
    "    - Probabilistic Latent Semantic Analysis (PLSA) - that was first proposed i 1999\n",
    "    - Latent Dirichlet Allocation (LDA) - was proposed in 2003\n",
    "- LDA is by far one the most popular topic models \n",
    "\n",
    "### Generative models and LDA\n",
    "- LDA is a geerative model,\n",
    "    - first, you decide what is the length of the document d\n",
    "    - then you choose amixture of topics for that document\n",
    "    - then you use the topics multinomial distribution-that is the  word distribution to output the words to fill up the quota, that topic's quota.\n",
    "- there are many packages available to perform topic modelling in python such as gansim, lda\n",
    "- before you use any of these you need first to pre-process text;\n",
    "    - you need to tokenize text and normalize it - that means make them all lowercase\n",
    "    - you remove stop words - stop words are coomon words that occur frequently in a particular domain but they are not meaningful in that domain. examples in general english are 'the', 'is' and 'are'. you might want to remove those. if you are in the area of medical document lets say so clinical notes, you would always see the word patient and doctor and so on and they may not be as important as other words say the disease or the medication. you might also want to categorize them as stop words\n",
    "    - the other pre-processing would be stemming-that means you would need to remove the derivation in related forms. somehow normalize the derivation in related forms to the same word. meet, met, meets all should be called meet.\n",
    "- once you have done the pre-processing steps, you convert this tokenized document into a document term matrix\n",
    "- oce you have the document term matrix, then you biuld the LDA models on top of it    \n",
    "- so onc you have built the mapping between the terms and documents then suppose you have a setof pre-processed text documents in this variable doc_set\n",
    "- then you could use genism to learn LDA.\n",
    "- lda can also be used to find the tpic distribution of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-256bcffee7dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 4, \n\u001b[0;32m      4\u001b[0m                                            id2word = dictionary, passes = 50)\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_set' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(doc_set)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_set]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 4, \n",
    "                                           id2word = dictionary, passes = 50)\n",
    "print(ldamodel.print_topics(num_topics = 4, num_words = 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take home concepts\n",
    "- topic modelling is an exploratory tool frequently used for text mining\n",
    "- latent Dirichlet Allocation(LDA) is a generative model used extensively for modelling large text corpora\n",
    "- LDA can also be used as a feature selection technique for text classification and other tasks\n",
    "\n",
    "## Information Extraction\n",
    "- information is hidden in free-text in very interesting ways.\n",
    "- most traditional transactional information is structured while a lot of information now is in unstructured free text form\n",
    "### fields of interest\n",
    "- fields of interest are named entities\n",
    "- if its news, we are talking about people, places, dates and organizations\n",
    "- if we are talking about finance we're talking about money, name of companies, stok price index or monetary values\n",
    "- if we are talking about medicine and health, we are talking about diseases, drugs and procedures\n",
    "- and especially if we are talking about protected health information we are talking about address and their unique identifiers\n",
    "- named entity recognition relies on something called named entities.\n",
    "- Named entities are noun phrases that are of specific type and refer to specific individuals, places, organizations and so on.\n",
    "### approaches to identify named entities\n",
    "- it really depends on the kind of entities that we have.\n",
    "- for example if we have well-formatted fields like dates and phone numbers then we would use regular expression\n",
    "- for other fields, its fairly common to use machine learning approach where you use these regular expressions as features but you have other features that help them define this particular extraction be valid.\n",
    "\n",
    "### Take home concepts\n",
    "- information extraction is important task for natural language understanding and making sense of textual data\n",
    "- it is the first step in converting this unstructured text into more structured form\n",
    "- named entity recognition is a key building block in addressing these tasks and these advanced NLP tasks \n",
    "- named Entity recognition systems extensively deploy supervised machine learning and text mining techniques earlier discussed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
