{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydotplus in c:\\users\\tonny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\tonny\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pydotplus) (2.4.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\tonny\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.executable\n",
    "!{sys.executable} -m pip install pydotplus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rem if we reduce k to say a value 1, we will get an overfitted model\n",
    "- in the k = 1 case, the training score is a perfect 1 and the test score is 80%\n",
    "- in the k = 3 case, the training score drops to 0.88 while the test score improves to 88%, indicating the model is generalizing better to new data\n",
    "- in the k = 11 case, the training score drops abit further to .80 while the test score improves to 92%, indicating that this simple model is much more effective at ignoring minor variations in the training data and instead capturing the more important global trend in where the classes tend to be located with the best overall generalization perfomance as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the nearest neighbors approach is not only used for classification but also for regression too\n",
    "- because the target values in a regression case are continous as compared to the discrete values in the classification case, to asses how well our regresion model fits the data we use a regression score called R-squared thats between zero and one\n",
    "- for the r-squared value, a value of 1 corresponds to the best possible performance, a model that makes the best predictions. A value 0 corresponds to a constant model that predicts the mean value of all training target values.\n",
    "- The r-squared value is sometimes known as the coefficient of determination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to model complexity in knn classification, small values of k give models with higher complexity  and large values of k give models with lower complexity.\n",
    "- in our example, \n",
    "- starting when k= 1, the regression model fits the training data perfectly with a r-squared score of 1.0 but its very bad at predicting the target values for new data sample as reflected in the r-squared score of  only 0.155 (15.5%)\n",
    "- as the value of k increases, which we see acts to smooth out these local variations to capture more of the global trend\n",
    "- with k = 3, the training test score drops to 0.797 while the test score increases to 0.323 (32.3%)  \n",
    "- with k = 7, the train test score drops to 0.720 while the test score increases to 0.471(47.1%)\n",
    "- the model with k = 15 has the best test perfomance with an r-squared score of 0.485 (48.5%). increasing k = 55, results in both the training score (35.7%) and the test score (37.1%) drop back down to lower levels as the model now starts to under-fit or the model gets too simple to do well, even on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the pros of the nearest neighbor approach are that its simple and easy to understand whya particular prediction is made.\n",
    "- A K-NN approach can be a reasonable baseline aganist what you can compare ore sophisticated methods.\n",
    "- When the training data has many instances, or each instance has a lot of features, this can really slow down the perfomance of a k-nn model. \n",
    "- in general, if your data has hundreds or thouands of features, you should consider alternatives to K-NN models, especially if your data is sparse-meaning that each instance has lots of features but most of them are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit80120bd8a8234beda8c48ae084af3c3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
