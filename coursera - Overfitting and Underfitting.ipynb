{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization, overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Ip\n",
    "**Generalization ability** refers to an algorithm's ability to give accurate predictions for new, previously unseen data.\n",
    "- **Assumptions**\n",
    "1. machine learning makes an important assumption, that the future test set has the same properties or is drawn from the same underlying distribution as our training set thus models that are accurate on the training set are expected to be accurate on the test set\n",
    "- that however might not happen if the trained model is tuned too specifically to the training set.\n",
    "\n",
    "- Models that are too complex for the amount of training data available are said to overfit and are not likely to generalize well to new examples\n",
    "- Models that are too simple, that dont even do well on the training data, are said to underfit and also not likely to generalize well.\n",
    "- understanding, detecting and avoiding overfitting is perhaps the most important aspect of applying supervised machine learning algorithms that one should master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of overfitting and underfitting in regression\n",
    "![Title](over_and_underfitting.png)\n",
    "- in the first plot the algorithm displays underfitting.\n",
    "- The second plot is a well fitted model\n",
    "- The third plot is a good example of an overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of underfitting and overfitting in classification\n",
    "![Random Unsplash Image](overfitting_2.png)\n",
    "\n",
    "- the first plot shows an underfitted model\n",
    "- the second plot shows a well fitted model\n",
    "- the third plot shows an overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### an example of overfitting with a k-nn classifier \n",
    "![Title](screenshot_overfitting.png)\n",
    "- a value k means the actual number of closest neighbors the classiifier has to look at before it classifies our new point\n",
    "- for the first image where k=10, this means that the classifier has to look at the nearest 10 points before it can make a decision.\n",
    "- for k=5 the classifier has to look at the 5 nearest neighbors before it has to make a decision to classify our unseen point\n",
    "- for the last image where k=1, our clasifier only looks at the nearest point to classify our unseen data set. \n",
    "- this actually means in some sense that we can control the degree of model fitting thats appropriate for a dataset.\n",
    "- the general idea is that as we decrease k for k-nn classifiers, we increase the risk of overfitting because where k=1 for example, we are trying to capture very local changes in the decision boundary that may not lead to a good generalization behavior for future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
